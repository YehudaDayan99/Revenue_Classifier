{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests)\n",
            "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "   ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
            "   -------- ------------------------------- 30.7/152.9 kB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 152.9/152.9 kB 2.3 MB/s eta 0:00:00\n",
            "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "   ---------------------------------------- 0.0/131.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 131.6/131.6 kB 7.6 MB/s eta 0:00:00\n",
            "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
            "Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.6.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/7] Processing AAPL...\n",
            "  AAPL: Fetching ticker->CIK map...\n",
            "  AAPL: Fetching submissions for CIK 320193...\n",
            "  AAPL: Found 10-K filed on 2025-10-31\n",
            "  AAPL: Downloading filing index...\n",
            "  AAPL: Downloading primary document (aapl-20250927.htm)...\n",
            "[1/7] ✓ AAPL completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\AAPL\\2025-10-31_000032019325000079\n",
            "[2/7] Processing MSFT...\n",
            "  MSFT: Fetching ticker->CIK map...\n",
            "  MSFT: Fetching submissions for CIK 789019...\n",
            "  MSFT: Found 10-K filed on 2025-07-30\n",
            "  MSFT: Downloading filing index...\n",
            "  MSFT: Downloading primary document (msft-20250630.htm)...\n",
            "[2/7] ✓ MSFT completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\MSFT\\2025-07-30_000095017025100235\n",
            "[3/7] Processing GOOGL...\n",
            "  GOOGL: Fetching ticker->CIK map...\n",
            "  GOOGL: Fetching submissions for CIK 1652044...\n",
            "  GOOGL: Found 10-K filed on 2025-02-05\n",
            "  GOOGL: Downloading filing index...\n",
            "  GOOGL: Downloading primary document (goog-20241231.htm)...\n",
            "[3/7] ✓ GOOGL completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\GOOGL\\2025-02-05_000165204425000014\n",
            "[4/7] Processing AMZN...\n",
            "  AMZN: Fetching ticker->CIK map...\n",
            "  AMZN: Fetching submissions for CIK 1018724...\n",
            "  AMZN: Found 10-K filed on 2025-02-07\n",
            "  AMZN: Downloading filing index...\n",
            "  AMZN: Downloading primary document (amzn-20241231.htm)...\n",
            "[4/7] ✓ AMZN completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\AMZN\\2025-02-07_000101872425000004\n",
            "[5/7] Processing NVDA...\n",
            "  NVDA: Fetching ticker->CIK map...\n",
            "  NVDA: Fetching submissions for CIK 1045810...\n",
            "  NVDA: Found 10-K filed on 2025-02-26\n",
            "  NVDA: Downloading filing index...\n",
            "  NVDA: Downloading primary document (nvda-20250126.htm)...\n",
            "[5/7] ✓ NVDA completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\NVDA\\2025-02-26_000104581025000023\n",
            "[6/7] Processing META...\n",
            "  META: Fetching ticker->CIK map...\n",
            "  META: Fetching submissions for CIK 1326801...\n",
            "  META: Found 10-K filed on 2025-01-30\n",
            "  META: Downloading filing index...\n",
            "  META: Downloading primary document (meta-20241231.htm)...\n",
            "[6/7] ✓ META completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\META\\2025-01-30_000132680125000017\n",
            "[7/7] Processing AVGO...\n",
            "  AVGO: Fetching ticker->CIK map...\n",
            "  AVGO: Fetching submissions for CIK 1730168...\n",
            "  AVGO: Found 10-K filed on 2025-12-18\n",
            "  AVGO: Downloading filing index...\n",
            "  AVGO: Downloading primary document (avgo-20251102.htm)...\n",
            "[7/7] ✓ AVGO completed: C:\\Users\\yehud\\Business Classification\\data\\10k\\AVGO\\2025-12-18_000173016825000121\n",
            "AAPL: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\AAPL\\2025-10-31_000032019325000079\n",
            "MSFT: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\MSFT\\2025-07-30_000095017025100235\n",
            "GOOGL: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\GOOGL\\2025-02-05_000165204425000014\n",
            "AMZN: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\AMZN\\2025-02-07_000101872425000004\n",
            "NVDA: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\NVDA\\2025-02-26_000104581025000023\n",
            "META: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\META\\2025-01-30_000132680125000017\n",
            "AVGO: OK - C:\\Users\\yehud\\Business Classification\\data\\10k\\AVGO\\2025-12-18_000173016825000121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# SEC compliance: include contact info\n",
        "os.environ[\"SEC_USER_AGENT\"] = \"RevenueSegBot/0.1 (your.email@domain.com)\"\n",
        "\n",
        "from revseg.sec_edgar import download_many_latest_10k\n",
        "\n",
        "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"AVGO\"]\n",
        "\n",
        "results = download_many_latest_10k(\n",
        "    tickers=tickers,\n",
        "    out_dir=Path(\"data/10k\"),\n",
        "    cache_dir=Path(\".cache/sec\"),\n",
        "    include_amendments=False,\n",
        "    min_interval_s=0.2,\n",
        ")\n",
        "\n",
        "for t, (ok, msg) in results.items():\n",
        "    print(f\"{t}: {'OK' if ok else 'FAIL'} - {msg}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting lxml\n",
            "  Using cached lxml-6.0.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
            "Collecting soupsieve>=1.6.1 (from beautifulsoup4)\n",
            "  Downloading soupsieve-2.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\yehud\\business classification\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
            "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
            "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
            "   -------------- ------------------------ 41.0/107.7 kB 960.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 107.7/107.7 kB 1.5 MB/s eta 0:00:00\n",
            "Using cached lxml-6.0.2-cp311-cp311-win_amd64.whl (4.0 MB)\n",
            "Downloading soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, lxml, beautifulsoup4\n",
            "Successfully installed beautifulsoup4-4.14.3 lxml-6.0.2 soupsieve-2.8.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for table extraction\n",
        "%pip install beautifulsoup4 lxml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(85,\n",
              " 'C:\\\\Users\\\\yehud\\\\Business Classification\\\\data\\\\table_candidates\\\\MSFT_table_candidates.json',\n",
              " 'data\\\\10k\\\\MSFT\\\\2025-07-30_000095017025100235\\\\primary_document.html')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "# If your SEC_USER_AGENT is already set in your environment, you can omit this.\n",
        "# os.environ[\"SEC_USER_AGENT\"] = \"RevenueSegBot/0.1 (your.email@domain.com)\"\n",
        "\n",
        "# Reload the module to pick up any changes\n",
        "import revseg.table_candidates\n",
        "importlib.reload(revseg.table_candidates)\n",
        "\n",
        "from revseg.table_candidates import (\n",
        "    find_latest_downloaded_filing_dir,\n",
        "    find_primary_document_html,\n",
        "    extract_table_candidates_from_html,\n",
        "    write_candidates_json,\n",
        ")\n",
        "\n",
        "ticker = \"MSFT\"\n",
        "base_dir = Path(\"data/10k\")\n",
        "\n",
        "filing_dir = find_latest_downloaded_filing_dir(base_dir, ticker)\n",
        "html_path = find_primary_document_html(filing_dir)\n",
        "\n",
        "candidates = extract_table_candidates_from_html(\n",
        "    html_path,\n",
        "    preview_rows=15,\n",
        "    preview_cols=8,\n",
        ")\n",
        "\n",
        "out_json = write_candidates_json(\n",
        "    candidates,\n",
        "    Path(f\"data/table_candidates/{ticker}_table_candidates.json\"),\n",
        ")\n",
        "\n",
        "(len(candidates), str(out_json), str(html_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 financial table candidates (ranked by financial table signals):\n",
            "\n",
            "t0073 year_header= True units= True money= 0.194 numeric= 0.194 years= [2023, 2024, 2025] labels= ['(In millions)', 'Year Ended June 30,', 'Server products and cloud services']\n",
            "t0042 year_header= True units= True money= 0.17 numeric= 0.149 years= [2023, 2024, 2025] labels= ['(In millions)', 'June 30,', 'Land']\n",
            "t0029 year_header= True units= True money= 0.154 numeric= 0.096 years= [2023, 2024, 2025] labels= ['(In millions)', 'Year Ended June 30,', 'Interest and dividends income']\n",
            "t0055 year_header= True units= True money= 0.151 numeric= 0.151 years= [2024, 2025] labels= ['(In millions)', 'June 30,', 'Deferred Income Tax Assets']\n",
            "t0018 year_header= True units= True money= 0.148 numeric= 0.148 years= [2024, 2025] labels= ['(In millions)', 'Year Ended June 30,', 'Interest and dividends income']\n",
            "t0050 year_header= True units= True money= 0.142 numeric= 0.142 years= [2009, 2010, 2021, 2023, 2024, 2026, 2030, 2039, 2040, 2050, 2052, 2060, 2062] labels= ['(In millions, issuance by calendar year)', '2009 issuance of $ 3.8 billion', '2010 issuance of $ 4.8 billion']\n",
            "t0056 year_header= True units= True money= 0.138 numeric= 0.138 years= [2023, 2024, 2025] labels= ['(In millions)', 'Year Ended June 30,', 'Beginning unrecognized tax benefits']\n",
            "t0013 year_header= True units= True money= 0.136 numeric= 0.136 years= [2024, 2025] labels= ['(In millions, except percentages and per share amounts)', 'Revenue', 'Gross margin']\n",
            "t0027 year_header= True units= True money= 0.135 numeric= 0.115 years= [2024, 2025] labels= ['(In millions, except per share amounts)', 'Year Ended June 30,', 'Common stock and paid-in capital']\n",
            "t0026 year_header= True units= True money= 0.132 numeric= 0.094 years= [2024, 2025] labels= ['(In millions)', 'Year Ended June 30,', 'Operations']\n"
          ]
        }
      ],
      "source": [
        "# Deterministic \"financial table\" ranking - prioritizes tables with year headers, units, and money cells\n",
        "ranked = sorted(\n",
        "    candidates,\n",
        "    key=lambda c: (\n",
        "        c.has_year_header,\n",
        "        c.has_units_marker,\n",
        "        c.money_cell_ratio,\n",
        "        c.numeric_cell_ratio,\n",
        "        len(c.keyword_hits),\n",
        "        c.n_rows * c.n_cols,\n",
        "    ),\n",
        "    reverse=True,\n",
        ")\n",
        "\n",
        "print(\"Top 10 financial table candidates (ranked by financial table signals):\\n\")\n",
        "for c in ranked[:10]:\n",
        "    print(\n",
        "        c.table_id,\n",
        "        \"year_header=\", c.has_year_header,\n",
        "        \"units=\", c.has_units_marker,\n",
        "        \"money=\", round(c.money_cell_ratio, 3),\n",
        "        \"numeric=\", round(c.numeric_cell_ratio, 3),\n",
        "        \"years=\", c.detected_years,\n",
        "        \"labels=\", c.row_label_preview[:3],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "table_id: t0071\n",
            "rows x cols: 45 x 13\n",
            "has_year_header: True\n",
            "detected_years: [2024, 2025]\n",
            "has_units_marker: True\n",
            "units_hint: (In millions)\n",
            "numeric_cell_ratio: 0.09259259259259259\n",
            "money_cell_ratio: 0.09259259259259259\n",
            "row_label_preview (first 10): ['(In millions)', 'Year Ended June 30,', 'Productivity and Business Processes', 'Revenue', 'Cost of revenue', 'Operating expenses', 'Operating Income']\n",
            "heading: \n",
            "caption: \n",
            "\n",
            "table_id: t0073\n",
            "rows x cols: 20 x 13\n",
            "has_year_header: True\n",
            "detected_years: [2023, 2024, 2025]\n",
            "has_units_marker: True\n",
            "units_hint: (In millions)\n",
            "numeric_cell_ratio: 0.1941747572815534\n",
            "money_cell_ratio: 0.1941747572815534\n",
            "row_label_preview (first 10): ['(In millions)', 'Year Ended June 30,', 'Server products and cloud services', 'Microsoft 365 Commercial products and cloud services', 'Gaming', 'LinkedIn', 'Windows and Devices', 'Search and news advertising', 'Dynamics products and cloud services', 'Enterprise and partner services']\n",
            "heading: \n",
            "caption: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find and print diagnostics for specific table_ids (the actual financial tables)\n",
        "def show(table_id: str):\n",
        "    c = next(x for x in candidates if x.table_id == table_id)\n",
        "    print(\"table_id:\", c.table_id)\n",
        "    print(\"rows x cols:\", c.n_rows, \"x\", c.n_cols)\n",
        "    print(\"has_year_header:\", c.has_year_header)\n",
        "    print(\"detected_years:\", c.detected_years)\n",
        "    print(\"has_units_marker:\", c.has_units_marker)\n",
        "    print(\"units_hint:\", c.units_hint)\n",
        "    print(\"numeric_cell_ratio:\", c.numeric_cell_ratio)\n",
        "    print(\"money_cell_ratio:\", c.money_cell_ratio)\n",
        "    print(\"row_label_preview (first 10):\", c.row_label_preview[:10])\n",
        "    print(\"heading:\", c.heading_context[:200])\n",
        "    print(\"caption:\", c.caption_text[:200])\n",
        "    print()\n",
        "\n",
        "show(\"t0071\")\n",
        "show(\"t0073\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yehud\\AppData\\Local\\Temp\\ipykernel_36112\\2417644637.py:11: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(html_path.read_text(encoding=\"utf-8\", errors=\"ignore\"), \"lxml\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nearest heading: \n",
            "Nearby context: ertising 13,878 12,306 12,125 Dynamics products and cloud services 7,827 6,831 5,796 Enterprise and partner services 7,760 7,594 7,900 Microsoft 365 Consumer products and cloud services 7,404 6,648 6,417 Other 72 45 119 Total $ 281,724 $ 245,122 $ 211,915 Our Microsoft Cloud revenue, which includes Microsoft 365 Commercial cloud, Azure and other cloud services, the commercial portion of LinkedIn, and Dynamics 365, was $ 168.9 billion, $ 137.7 billion, and $ 111.6 billion in fiscal years 2025, 2024, and 2023, respectively. These amounts are included in Microsoft 365 Commercial products and cloud services, Server products and cloud services, LinkedIn, and Dynamics products and cloud services in the table above. Revenue, classified by significant product and service offerings, was as follows:\n",
            "\n",
            "--- Table text (first ~40 lines) ---\n",
            "(In millions) Year Ended June 30, 2025 2024 2023 Server products and cloud services $ 98,435 $ 79,828 $ 65,007 Microsoft 365 Commercial products and cloud services 87,767 76,969 66,949 Gaming 23,455 21,503 15,466 LinkedIn 17,812 16,372 14,989 Windows and Devices 17,314 17,026 17,147 Search and news advertising 13,878 12,306 12,125 Dynamics products and cloud services 7,827 6,831 5,796 Enterprise and partner services 7,760 7,594 7,900 Microsoft 365 Consumer products and cloud services 7,404 6,648 6,417 Other 72 45 119 Total $ 281,724 $ 245,122 $ 211,915\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "import revseg.table_candidates as tc\n",
        "\n",
        "ticker = \"MSFT\"\n",
        "base_dir = Path(\"data/10k\")\n",
        "\n",
        "filing_dir = tc.find_latest_downloaded_filing_dir(base_dir, ticker)\n",
        "html_path = tc.find_primary_document_html(filing_dir)\n",
        "\n",
        "soup = BeautifulSoup(html_path.read_text(encoding=\"utf-8\", errors=\"ignore\"), \"lxml\")\n",
        "tables = soup.find_all(\"table\")\n",
        "\n",
        "table_id = \"t0073\"\n",
        "idx = int(table_id[1:])  # 73\n",
        "tbl = tables[idx]\n",
        "\n",
        "print(\"Nearest heading:\", tc._nearest_heading_text(tbl))\n",
        "print(\"Nearby context:\", tc._collect_nearby_text(tbl)[:1200])\n",
        "\n",
        "print(\"\\n--- Table text (first ~40 lines) ---\")\n",
        "lines = tc._clean_text(tbl.get_text(\"\\n\", strip=True)).split(\"\\n\")\n",
        "for line in lines[:40]:\n",
        "    print(line)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.11.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
